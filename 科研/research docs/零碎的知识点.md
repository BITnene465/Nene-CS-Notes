## 对比学习 （contrastive learning）

#### **什么是对比学习（Contrastive Learning）？**

目的：学习一个编码器，这个编码器对同类数据进行相似的编码，不同类的数据编码结果尽可能不同。

其核心思想是通过对比相似（正样本对）和不相似（负样本对）的数据点来学习数据的表示。它通常被归类为**无监督或自监督学习方法**，无需依赖人工标注的数据，而是通过最大化正样本对的相似性、最小化负样本对的相似性来捕捉数据间的结构和关系。



#### **为什么对比学习可以自监督？机器怎么知道哪些是正样本，哪些是负样本？**

对比学习无需人类标注即可确定正负样本，主要依赖数据增强 和数据分布特性 自动构建正负对。具体方法如下：

1. **正样本通过数据增强生成：**对同一原始数据进行不同变换（如裁剪、旋转、噪声注入等），生成的增强样本被视为正样本对。例如，一张图片经过随机裁剪和颜色抖动后得到的两个版本，会被模型视为“同类”正样本对。这种策略基于假设：同一数据的不同增强版本应具有相似的语义特征。
2. **负样本通过批次内随机采样或全局分布获取：**在训练过程中，一个样本的“负样本”可以是同一批次（batch）中其他不同数据的增强版本。也可以直接从整个数据集中随机选取样本作为负样本。这种方法假设随机样本与当前样本的语义不相关。
3. **对比损失函数引导模型学习**：模型通过对比损失（如InfoNCE Loss）最大化正样本对的相似度、最小化负样本对的相似度。即使没有人工标签，这种“相对关系”的学习目标仍能驱动模型捕捉数据内在结构。

无监督的对比学习虽然可能引入噪声，但是效果不错。



#### **对比损失的相关公式有哪些？如何直观地理解？**

对比学习中常见的损失函数包括 **Contrastive Loss** 和 **InfoNCE Loss** .

**Contrastive Loss** 的公式为：  
$$
\mathcal{L} = \frac{1}{N} \sum_{n=1}^N \left[ y \cdot D_w^2 + (1 - y) \cdot \max(m - D_w, 0)^2 \right]
$$
其中 $ y $ 表示样本对的标签（$ y=1 $ 为负样本对，$ y=0 $ 为正样本对），$ D_w $ 是特征向量的欧氏距离，$ m $ 是预设的边界值（margin）。  
- **直观理解**：  
  - 对正样本对（$ y=0 $），希望其特征距离 $ D_w $ 趋近于0，即“同类样本在嵌入空间中聚集”；  
  - 对负样本对（$ y=1 $），要求其距离至少大于 $ m $，否则损失值增大，即“异类样本被推开”。  
  - 这类似于“橡皮筋和弹簧”的物理模型：正样本对之间用橡皮筋拉近，负样本对之间用弹簧推开，最终形成聚类结构。



**InfoNCE Loss** 的公式为：  
$$
\mathcal{L} = -\frac{1}{K} \sum_{k=1}^K \log \frac{\exp(\text{sim}(q, k^+)/\tau)}{\sum_{i=1}^N \exp(\text{sim}(q, k_i)/\tau)}
$$
其中 $ q $ 是查询样本，$ k^+ $ 是正样本，$ k_i $ 是负样本，$ \tau $ 是温度系数（temperature scaling）。  
- **直观理解**：  
  - 将正样本视为“正确类别”，负样本视为“干扰项”，通过 softmax 归一化实现多分类优化，最大化查询样本与正样本的相似度，同时最小化与负样本的相似度；  
  - 温度系数 $ \tau $ 控制相似度分布的锐化程度，较小的 $ \tau $ 会放大差异，使模型更关注难例负样本。  
  - 可类比为“寻宝游戏”：查询样本需在一堆负样本中找到唯一正确的正样本，完成任务。

这两种损失函数的核心目标一致：通过数学形式化实现“同类拉近、异类推开”，从而学习到具有判别性的特征表示。





## VAE

变分自编码器， Variational autoencoder