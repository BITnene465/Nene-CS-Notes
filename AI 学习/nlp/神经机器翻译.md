# 神经机器翻译

## RNN-NMT

NLP = NLU + NLG，  而这两部分， RNN都可以做到

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210051507999.png" alt="image-20241210051507999" style="zoom: 80%;" />

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210051403316.png" alt="image-20241210051403316" style="zoom:80%;" />

encoder-decoder 结构， 用于机器翻译





## Attention RNN-NMT

### Attention 机制

在普通 RNN 的基础上加入 **Attention 机制**

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210052110630.png" alt="image-20241210052110630" style="zoom:80%;" />



对齐网络 a 可更换



### GNMT

堆叠LSTM -->  增加宽度

**stacked LSTM**



<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210102043167.png" alt="image-20241210102043167" style="zoom:80%;" />







## Transformer

进一步实现了**并行计算**,解决**长期依赖**

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210110650029.png" alt="image-20241210110650029" style="zoom:67%;" />

### Self-attention

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210103608697.png" alt="image-20241210103608697" style="zoom:67%;" />



<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210104020966.png" alt="image-20241210104020966" style="zoom:67%;" />

两个重要超参： 

- embedding dimension  -- 决定理解能力
- seq length  -- 决定最大可输入 token 数



num of heads -- 





<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210110112678.png" alt="image-20241210110112678" style="zoom:67%;" />

- self-attention 用于理解

- feed forward 网络 用于拟合 (也可以改成残差网络)





### masked self-attention

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20241210112109986.png" alt="image-20241210112109986" style="zoom:80%;" />



### cross attention





### Position Embedding

由于 transformer 去除了RNN，所有词之间的距离都为1，所以需要一个位置向量来记录时序关系。

没有位置编码 Transformer 就是BOW（词袋模型）



- Transformer的Sinusoidal位置编码
- BERT等的**learnable positional embedding**
- LLaMA等的**RoPE**(旋转位置编码)



### 思考题

- Multi-head self-attention带来的问题
- Self-attention中的Softmax是不是必须的？

- Transformer架构是否要求定长输入?
- Transformer中有参数共享部分吗？
- Transformer层中的前馈网络的作用？
- 可否处理其他模态？



#### 当前架构

为解决 self-attention 的二次复杂度

- Mamba
- MiniLSTM
- MiniGRU



#### 两个向量的交互

双线性变换

双仿射变换

