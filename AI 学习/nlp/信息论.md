# 信息论

> nlp 中使用的一点点信息论



## 香农熵

> ***“香农熵是一种基本的限制，即我们可以在多大程度上压缩一个信息来源，而不会使其有失真或丢失的风险。”***







## 互信息 (*mutual information, MI*)

技术例子：

- WordPiece （Bert 使用的 tokenization 技术）



<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E4%BF%A1%E6%81%AF%E8%AE%BA/image-20241022115042856.png" alt="image-20241022115042856" style="zoom:80%;" />



## 交叉熵(*cross entropy*)

例子：

- nlp 中的深度学习多使用交叉熵损失函数


$$
H(X, q) = -\sum_{x \in X} p(x) \ log \ q(x)
$$
<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E4%BF%A1%E6%81%AF%E8%AE%BA/image-20241022114512271.png" alt="image-20241022114512271" style="zoom: 80%;" />



## 相对熵（*Kullback-Leibler divergence, KL 散度*）

> ***(0/1)二分类下，交叉熵就等于相对熵***

$$
D(p \ || \ q) = \sum_{x \in X} p(x) log\frac{p(x)}{q(x)} = H(X, q) - H(X)
$$



![image-20241022114658569](G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E4%BF%A1%E6%81%AF%E8%AE%BA/image-20241022114658569.png)





## 困惑度(perplexity)

> ***一般我们希望模型的熵越小越好， 因为熵更小，各类别的概率分布差别越大，答案更加确定。***

应用：

- 语言模型评价指标

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E4%BF%A1%E6%81%AF%E8%AE%BA/image-20241022115506934.png" alt="image-20241022115506934" style="zoom:80%;" />







## 最大熵(maximum entropy)

### 思想

**最大熵的思想符合直觉，直觉上：**

1. 满足全部已知的条件
2. 对未知的情况不做任何主观假设（随机的）

**基本要求**：

- **Uniform distribution**
- **Make entropy maximum**

**总结**： 有证据就用，没有就认为它是随机的 —— 因为不随机的你已经考虑完了，如果再考虑非随机分布你就没有理由了，不是最好地模拟已知或世界了。



### 最大熵模型（*ME*）

<img src="G:/softwares/typora/typora%20%E5%9B%BE%E7%89%87/%E4%BF%A1%E6%81%AF%E8%AE%BA/image-20241022120026473.png" alt="image-20241022120026473" style="zoom:80%;" />







## 信息瓶颈 (information bottleneck)























