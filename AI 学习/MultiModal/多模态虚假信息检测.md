> **高校计算机能力挑战赛 2024 人工智能赛道**



## solution

要解决多模态虚假信息检测问题，涉及文本和图像的输入，并采用注意力机制进行整合。以下是一个基于 PyTorch 框架的解决方案，文本部分使用 BERT 提取特征，图像部分使用 ResNet 提取特征，最后通过 Transformer 的注意力机制来整合这两部分特征。

### 设计 dataset

```python
import torch
from torch.utils.data import Dataset
from transformers import BertTokenizer
import cv2 as cv
import numpy as np
import requests

class MultiModalDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128, transform=None):
        """
        初始化自定义数据集
        :param dataframe: 包含文本、图片和标签的数据框
        :param tokenizer: BERT的tokenizer用于文本处理
        :param max_length: 文本序列的最大长度
        :param transform: 图像预处理（传入None时使用cv2进行图像处理）
        """
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.transform = transform
    
    def __len__(self):
        return len(self.dataframe)
    
    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        
        # 获取标题和内容
        title = row['Title']
        report_content = row['Report Content']
        text_input = f"{title} {report_content}"  # 将标题和内容结合
        
        # 文本处理：BERT tokenizer
        encoding = self.tokenizer(
            text_input,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        
        input_ids = encoding['input_ids'].squeeze(0)  # 处理后的token ids
        attention_mask = encoding['attention_mask'].squeeze(0)  # 注意力掩码
        
        # 图像处理（使用cv和requests）
        image_url = row['Image Url']
        response = requests.get(image_url)  # 获取图像的响应
        image_array = np.asarray(bytearray(response.content), dtype=np.uint8)  # 将响应内容转为numpy数组
        image = cv.imdecode(image_array, cv.IMREAD_COLOR)  # 解码为OpenCV图像
        
        # 调整图像大小为224x224，并进行归一化
        image = cv.resize(image, (224, 224))
        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)  # 转换为RGB模式
        image = image / 255.0  # 归一化到[0,1]
        image = np.transpose(image, (2, 0, 1))  # 转换为(C, H, W)格式
        image = torch.tensor(image, dtype=torch.float32)  # 转换为tensor
        
        # 标签
        label = torch.tensor(row['label']).long()
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'image': image,
            'label': label
        }
```



使用示例：

```python
# 加载BERT tokenizer
tokenizer = BertTokenizer.from_pretrained(r'G:\pretrainedModel\pytorch\Bert\bert-base-chinese')

# 读取你的数据集
df = pd.read_csv('./虚假新闻检测/train/train.csv')

# 初始化自定义数据集
dataset = MultiModalDataset(dataframe=df, tokenizer=tokenizer)

# 使用DataLoader进行批次化
from torch.utils.data import DataLoader

dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 查看一个batch
for batch in dataloader:
    print(batch['input_ids'].shape)   # (batch_size, max_length)
    print(batch['attention_mask'].shape)  # (batch_size, max_length)
    print(batch['image'].shape)  # (batch_size, 3, 224, 224)
    print(batch['label'].shape)  # (batch_size,)
    break
```





### 设计模型结构

以下是一个简化版的模型实现示例，包含特征提取和注意力机制整合部分：

#### 文本特征提取

```python
from transformers import BertModel

class TextFeatureExtractorBERT(torch.nn.Module):
    def __init__(self, model_path=None, device='cpu'):
        super(TextFeatureExtractorBERT, self).__init__()
        # 如果提供了预训练模型路径，就加载本地模型
        if model_path:
            self.bert = BertModel.from_pretrained(model_path)
        else:
            self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.device = device
        self.bert.to(self.device)
    
    def forward(self, input_ids, attention_mask):
        # 获取文本的最后一层隐藏状态
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]
        # 使用池化后的CLS token作为文本的特征向量
        cls_embedding = last_hidden_state[:, 0, :]  # [batch_size, hidden_size]
        return cls_embedding
```



#### 图像特征提取

```python
import torch
import torch.nn as nn
from torchvision import models

class ImageFeatureExtractorResNet(torch.nn.Module):
    def __init__(self, model_path=None, device='cpu'):
        super(ImageFeatureExtractorResNet, self).__init__()
        # 加载ResNet50模型并移除最后的全连接层
        self.resnet = models.resnet50(pretrained=False)
        if model_path:
            self.resnet.load_state_dict(torch.load(model_path, map_location=device))
        else:
            self.resnet = models.resnet50(pretrained=True)
        
        # 修改ResNet的最后一个层，去掉分类头
        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])
        self.device = device
        self.resnet.to(self.device)

    def forward(self, images):
        # 输入图像，返回特征向量
        features = self.resnet(images)
        features = features.view(features.size(0), -1)  # 展平 [batch_size, 2048]
        return features
```



#### 多模态融合 + 注意力机制

```python
import torch.nn as nn

class MultiModalTransformer(nn.Module):
    def __init__(self, text_feature_dim=768, image_feature_dim=2048, hidden_dim=512, num_classes=2, num_heads=8, num_layers=2, dropout=0.1, model_path=None, device='cpu'):
        super().__init__()
        
        # 线性层将图像和文本特征映射到统一维度
        self.text_fc = nn.Linear(text_feature_dim, hidden_dim)
        self.image_fc = nn.Linear(image_feature_dim, hidden_dim)
        
        # 加入LayerNorm和Dropout以稳定训练
        self.text_norm = nn.LayerNorm(hidden_dim)
        self.image_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
        
        # Transformer encoder层用于融合多模态特征
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # 分类层
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
        # 初始化模型权重
        if model_path:
            self.load_state_dict(torch.load(model_path))
        
        self.device = device
        self.to(self.device)

    def forward(self, text_features, image_features):
        # 将文本和图像特征映射到相同维度
        text_features = self.text_norm(self.dropout(self.text_fc(text_features)))  # [batch_size, hidden_dim]
        image_features = self.image_norm(self.dropout(self.image_fc(image_features)))  # [batch_size, hidden_dim]
        
        # 拼接文本和图像特征作为Transformer的输入
        combined_features = torch.stack([text_features, image_features], dim=0)  # [2, batch_size, hidden_dim]
        
        # 输入到Transformer Encoder
        fused_features = self.transformer_encoder(combined_features)  # [2, batch_size, hidden_dim]
        
        # 取第一个token的输出作为融合后的特征
        fused_output = fused_features[0, :, :]  # [batch_size, hidden_dim]
        
        # 最终分类
        logits = self.classifier(fused_output)  # [batch_size, num_classes]
        return logits
```



#### 整合模型

```python
class MultiModalModel(nn.Module):
    def __init__(self, text_model_path=None, image_model_path=None, transformer_model_path=None, device='cpu'):
        super().__init__()
        # 初始化文本特征提取器
        self.text_extractor = TextFeatureExtractorBERT(model_path=text_model_path, device=device)
        
        # 初始化图像特征提取器
        self.image_extractor = ImageFeatureExtractorResNet(model_path=image_model_path, device=device)
        
        # 初始化多模态Transformer融合模型
        self.transformer = MultiModalTransformer(model_path=transformer_model_path, device=device)

    def forward(self, input_ids, attention_mask, images):
        # 获取文本特征
        text_features = self.text_extractor(input_ids=input_ids, attention_mask=attention_mask)
        
        # 获取图像特征
        image_features = self.image_extractor(images)
        
        # 将文本和图像特征输入到Transformer进行融合
        logits = self.transformer(text_features, image_features)
        return logits
```





#### 使用示例



```python
# 模型路径
text_model_path = r'G:\pretrainedModel\pytorch\Bert\bert-base-chinese'
image_model_path = r'G:\pretrainedModel\pytorch\hub\checkpoints\resnet50-0676ba61.pth'
transformer_model_path = None    # 无需预训练权重，直接训练即可

# 初始化模型并指定设备
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = MultiModalModel(text_model_path=text_model_path, image_model_path=image_model_path, transformer_model_path=transformer_model_path, device=device)

# 示例输入
input_ids = torch.randint(0, 1000, (32, 128)).to(device)  # 模拟的input_ids
attention_mask = torch.ones(32, 128).to(device)   # 模拟的attention_mask
images = torch.randn(32, 3, 224, 224).to(device)   # 模拟的图像数据

# 前向传播
logits = model(input_ids, attention_mask, images)
print(logits.shape)     # 输出 [batch_size, num_classes]
```





### 训练流程

为了使用这个模型，你可以定义训练循环，将输入的文本、图像、标签传入模型进行训练。







### 结论

这种模型通过 Bert 和 ResNet 提取特征，随后使用 Transformer 的注意力机制来融合文本和图像的特征，并最终通过一个全连接层进行分类。这个框架非常灵活，可以根据任务的需要进一步调整模型结构、优化超参数等。





## others

### 获取模型参数总数

```python
for name, param in model.named_parameters():
    print(f"Parameter name: {name}, Shape: {param.shape}")
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params}")
```

