# 梯度下降学习笔记

## 什么是梯度下降法 (Gradient Descent)

梯度下降法（Gradient Descent），也常被称为最速下降法（Steepest Descent），是一种通过迭代来寻找目标函数最小值的优化算法。它在机器学习和人工智能领域被广泛用于递归地逼近最小偏差模型 。

### 核心思想

其核心思想是沿着函数梯度（或导数）下降最快的方向，一步步迭代更新参数，从而逼近函数的最小值点。

- **目标函数 (J)**: 我们希望最小化的函数，在机器学习中通常是**损失函数** 。
- **参数 ($\theta$)**: 目标函数中需要优化的变量 。
- **导数 ( $\frac{dJ}{d\theta}$ )**:
  - 在几何上，代表函数在该点切线的斜率 。
  - 表示参数 $\theta$ 的单位变化所引起的函数 J 的相应变化 。
  - 导数的方向代表函数 J 增大的方向 。

为了找到函数的最小值，我们需要沿着导数的**反方向**进行更新 。

### 更新公式

梯度下降的参数更新公式为：

$$
\theta_{new} = \theta_{old} - \eta \frac{dJ}{d\theta}
$$

- **$\eta$ (eta)**: **学习率 (Learning Rate)**，是一个超参数，它决定了每次迭代更新的步长 。
  - **$\eta$ 过小**: 会导致收敛速度非常慢 。
  - **$\eta$ 过大**: 可能会导致更新步子迈得太大，从而在最小值点附近来回震荡，甚至导致无法收敛 。

## 梯度下降法实现步骤

1.  **初始化**: 随机给定一个初始点、设定学习率 (步长) 和一个很小的阈值 。
2.  **计算梯度**: 计算函数在当前点的导数（梯度） 。
3.  **更新参数**: 根据梯度下降公式 $\theta = \theta - \text{学习率} \cdot \text{导数}$ 更新参数 。
4.  **计算差值**: 计算更新前后两点函数值的差值 。
5.  **判断收敛**: 如果差值小于设定的阈值，则认为已经找到极值点，停止迭代；否则，重复步骤 2-5 。

### 局部最优解 vs 全局最优解

对于非凸函数（即存在多个极值点的函数），梯度下降法可能会收敛到一个**局部最优解**，而非**全局最优解** 。

**解决方法**：

- 多次运行算法，每次从一个随机的不同初始点开始 。
- 梯度下降的初始点也是一个超参数 。

## 代码实践：求函数极值

以求解函数 $y = (x - 2.5)^{2} - 1$ 的极值为例 。

**1.  定义函数及其导数**

```python
# 阈值
epsilon = 1e-8
# 学习率
eta = 0.1

# 定义原函数
def J(theta):
    return (theta - 2.5)**2 - 1

# 定义导数
def dJ(theta):
    return 2 * (theta - 2.5)
```



**2. 封装梯度下降算法**

```python
theta_history = []
def gradient_descent(initial_theta, eta, epsilon=1e-8):
    theta = initial_theta
    theta_history.append(initial_theta)
    
    while True:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient # 更新参数
        theta_history.append(theta)
        
        # 判断是否收敛
        if (abs(J(theta) - J(last_theta)) < epsilon):
            break
```



**3. 处理学习率过大的问题**

如果学习率设置得过大（例如 eta = 1.1），计算结果可能会超出计算机表示范围，导致 OverflowError 。

**解决方法**:

- 修改目标函数

  ```python
def J(theta):
    try:
        return (theta - 2.5)**2 - 1
    except:
        return float('inf')
  ```

- 增加最大迭代次数: 在梯度下降函数中增加一个最大迭代次数 



## 梯度下降法与最小二乘法在一元线性回归中的应用

### 场景

给定一组数据点 (X, y)，求解一元线性回归方程 $y = w_0 + w_1 x$ 的参数 $w_0$ 和 $w_1$ 。



### 方法一：最小二乘法 (OLS)

最小二乘法通过最小化平方损失函数 $f=\sum_{i=1}^{n}(y_{i}-(w_{0}+w_{1}x_{i}))^{2}$ 来求解参数 7。通过对 $w_0$ 和 $w_1$ 分别求偏导数并令其为 0，可以得到参数的解析解 ：



$$
w_{1}=\frac{n\sum x_{i}y_{i}-\sum x_{i}\sum y_{i}}{n\sum x_{i}^{2}-(\sum x_{i})^{2}}
$$

$$
w_{0}=\frac{\sum x_{i}^{2}\sum y_{i}-\sum x_{i}\sum x_{i}y_{i}}{n\sum x_{i}^{2}-(\sum x_{i})^{2}}
$$

这种方法可以直接计算出精确解 。



### 方法二：梯度下降法

使用梯度下降法，我们需要根据损失函数的偏导数来迭代更新参数 $w_0$ 和 $w_1$ 。损失函数偏导数如下 ：

$$
\frac{\partial f}{\partial w_{0}}=-2\sum_{i=1}^{n}(y_{i}-(w_{0}+w_{1}x_{i}))
$$

$$
\frac{\partial f}{\partial w_{1}}=-2\sum_{i=1}^{n}x_{i}(y_{i}-(w_{0}+w_{1}x_{i}))
$$

参数的迭代更新公式为 ：

$$
w_{0} = w_{0} - lr \cdot \frac{\partial f}{\partial w_{0}}
$$

$$
w_{1} = w_{1} - lr \cdot \frac{\partial f}{\partial w_{1}}
$$

通过足够多的迭代次数，梯度下降法求解的参数会逐渐逼近最小二乘法的结果 。



### 对比与选择

| **特性**       | **最小二乘法 (OLS)**                     | **梯度下降法**                                     |
| -------------- | ---------------------------------------- | -------------------------------------------------- |
| **求解方式**   | 解析解，一次性计算                       | 迭代求解，逐步逼近                                 |
| **计算复杂度** | 当训练数据集过于庞大时，求解过程非常耗时 | 适合更复杂的算法或更庞大的训练集                   |
| **适用场景**   | 特征变量小于 $10^4$ 时，结果较稳妥       | 特征变量大于 $10^4$ 时，应使用梯度下降法降低计算量 |

**结论**: 一般而言，当特征数量不多时，推荐使用最小二乘法以获得精确解。当处理大规模数据或复杂模型时，梯度下降法是更常用和高效的选择。