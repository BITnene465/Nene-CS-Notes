# 聚类算法学习笔记

---

## 1. 聚类的基本概念

### 1.1 什么是聚类？

聚类（Clustering）是一种**无监督学习**方法。它的目标是按照特定的标准（如距离）将一个数据集分割成不同的类或簇。

**核心目标**：
* **簇内相似性最大化**：同一个簇内的数据对象尽可能相似。
* **簇间差异性最大化**：不同簇之间的数据对象尽可能不同。

### 1.2 聚类与分类的区别

* **聚类 (Clustering)**：无监督学习，不需要预先标记好的训练数据。算法自行发现数据中的结构和类别。
* **分类 (Classification)**：监督学习，需要一个带有类别标签的训练集来“学习”如何对未知数据进行分类。

---

## 2. 数据相似性度量

在聚类分析中，如何定义两个数据点之间的“相似”或“不相似”至关重要。这通常通过距离或相似性函数来完成。一个好的距离度量应该能够准确地反映数据点在特征空间中的真实接近程度。

| 类型 | 距离度量方法 |
| :--- | :--- |
| **常用距离** | 欧氏距离、曼哈顿距离、切比雪夫距离、闵可夫斯基距离 |
| **高级度量** | 马氏距离、标准化欧氏距离、余弦距离、相关距离 |
| **集合/信息度量** | 杰卡德距离、海明距离、信息熵 |
| **其他** | 基于核函数的度量 |



### 2.1 基于向量空间模型的距离

**这类方法通常用于衡量连续型数值特征。**

#### 2.1.1 欧氏距离 (Euclidean Distance)

* **解释**：在多维空间中，两点之间的**直线距离**。这是最常用、最直观的距离度量。
* **公式**：对于n维空间中的两个点 $A=(x_1, x_2, ..., x_n)$ 和 $B=(y_1, y_2, ..., y_n)$，它们之间的欧氏距离为：
    $$ d(A,B) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2} $$
* **应用场景**：适用于大多数特征维度量纲相同或已经过标准化的场景。是K-Means算法的默认距离度量。
* **优缺点**：
    * **优点**：直观，应用广泛。
    * **缺点**：没有考虑特征之间的相关性。对特征的量纲（单位）非常敏感，在使用前通常需要进行**标准化**处理。

#### 2.1.2 曼哈顿距离 (Manhattan Distance)

* **解释**：也称为“城市街区距离”（City Block Distance）。它计算的是两点在标准坐标系上**沿着轴线移动的距离总和**。想象在像曼哈顿这样的棋盘式街道上，从A点到B点只能沿着街道（坐标轴）行走的最短距离。
* **公式**：
    $$ d(A,B) = \sum_{i=1}^{n}|x_i - y_i| $$
* **应用场景**：适用于特征维度不相关，且特征维度具有实际物理意义的情况（例如棋盘格游戏或城市路线规划）。
* **优缺点**：
    * **优点**：计算比欧氏距离更快；对个别异常值没有欧氏距离那么敏感。
    * **缺点**：只能处理坐标轴相互垂直的情况，忽略了对角线方向的移动。

#### 2.1.3 闵可夫斯基距离 (Minkowski Distance)

* **解释**：这不是一个具体的距离，而是一个**距离范式**，是欧氏距离和曼哈顿距离的推广形式。
* **公式**：
    $$ d(A,B) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{\frac{1}{p}} $$
* **参数 p 的影响**:
    * 当 **p = 1** 时，就是**曼哈顿距离**。
    * 当 **p = 2** 时，就是**欧氏距离**。
    * 当 **p → ∞** 时，就是**切比雪夫距离 (Chebyshev Distance)**，即各个坐标距离中的最大值：$d(A,B) = \max_{i}(|x_i - y_i|)$。

#### 2.1.4 余弦相似度与余弦距离 (Cosine Similarity & Distance)

* **解释**：衡量两个向量在方向上的差异。余弦相似度计算的是两个向量夹角的余弦值。它不关心向量的绝对大小（长度），只关心它们指向的方向。
* **公式** (余弦相似度):
    $$ \text{similarity}(A,B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^{n}x_i y_i}{\sqrt{\sum_{i=1}^{n}x_i^2} \sqrt{\sum_{i=1}^{n}y_i^2}} $$
* **余弦距离**：通常由相似度转换而来：$d(A,B) = 1 - \text{similarity}(A,B)$。
* **应用场景**：非常适用于**文本挖掘**和**高维数据**。例如，计算两篇文档的相似度，每个词的TF-IDF值可以构成一个高维向量。
* **优缺点**：
    * **优点**：对向量的长度不敏感，只关注方向，在高维稀疏数据上表现良好。
    * **缺点**：忽略了数值的绝对大小。例如，向量 (1, 1) 和 (100, 100) 的余弦相似度为1，但它们的尺度差异巨大。



### 2.2 面向离散或集合数据的度量

这类方法主要用于衡量非数值型数据或集合。

#### 2.2.1 杰卡德相似系数与杰卡德距离 (Jaccard Index & Distance)

* **解释**：衡量两个集合之间的相似性。杰卡德相似系数计算的是两个集合**交集的大小**与**并集的大小**之比。
* **公式** (杰卡德相似系数):
    $$ J(A,B) = \frac{|A \cap B|}{|A \cup B|} $$
* **杰卡德距离**：由相似系数转换而来：$d(A,B) = 1 - J(A,B) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|}$。
* **应用场景**：计算用户之间的共同喜好（如共同购买的商品），或比较两篇文档中共同出现的词汇。
* **优缺点**：
    * **优点**：概念简单，易于计算。
    * **缺点**：不关心元素出现的具体次数或顺序，只关心是否存在。

#### 2.2.2 海明距离 (Hamming Distance)

* **解释**：衡量两个**等长字符串**之间对应位置上不同字符的数量。
* **公式**：对于两个等长的字符串A和B，$d(A,B)$ 等于A和B对应位置上字符不同的个数。例如，`"karolin"` 和 `"kathrin"` 的海明距离是3。对于二进制串，它就是两个串异或(XOR)后1的个数。
* **应用场景**：信息论、编码理论、密码学中比较二进制串或字符串的差异。
* **优缺点**：
    * **优点**：概念清晰，实现简单。
    * **缺点**：要求比较的两个数据对象长度必须一致。



### 2.3 考虑数据分布的距离

#### 2.3.1 马氏距离 (Mahalanobis Distance)

* **解释**：一种考虑了**特征之间相关性**并且**尺度无关 (scale-invariant)** 的距离度量。它通过协方差矩阵来度量两点之间的距离，可以看作是标准化的欧氏距离。
* **公式**：对于两个向量A和B，以及数据集的协方差矩阵 $S$，马氏距离为：
    $$ d(A,B) = \sqrt{(A - B)^T S^{-1} (A - B)} $$
* **应用场景**：异常点检测。当特征之间存在相关性时，使用马氏距离比欧氏距离更有效。
* **优缺点**：
    * **优点**：不受量纲影响，并排除了特征间的相关性干扰。
    * **缺点**：协方差矩阵的计算和求逆可能非常耗时，尤其是在高维情况下。

---

## 3. 主要聚类算法类别

聚类算法通常可以分为以下几类：

* **划分方法 (Partitioning Methods)**：将数据集划分为k个簇，代表算法是K-Means。
* **层次方法 (Hierarchical Methods)**：将数据对象建立一棵聚类树，可以是自底向上的凝聚策略，也可以是自顶向下的分裂策略。
* **基于密度的方法 (Density-Based Methods)**：将簇视为被低密度区域分割开的高密度区域，代表算法是DBSCAN。
* **基于网格的方法 (Grid-Based Methods)**：将数据空间量化为网格单元，在网格结构上进行聚类操作。
* **基于模型的方法 (Model-Based Methods)**：假设数据是根据某个潜在的概率分布生成的，并试图找到这个模型。

---

## 4. 经典聚类算法详解

### 4.1 K-Means (基于划分)

#### 算法思想

K-Means的目标是将n个对象划分为k个簇，使得簇内相似度高，而簇间相似度低。它通过最小化**平方误差函数**来作为其准则。

**准则函数 (平方误差)**:
$$ E=\sum_{i=1}^{k}\sum_{p\in C_{i}}|p-m_{i}|^{2} $$
其中，$p$ 是数据点，$C_i$ 是第 $i$ 个簇，$m_i$ 是簇 $C_i$ 的均值（质心）。

#### 算法步骤

1.  **初始化**：从数据集D中随机选择k个对象作为初始的簇中心（质心）。
2.  **分配 (Assignment / E-step)**：对于数据集中的每个对象，计算它与k个质心的距离，并将其分配到距离最近的那个簇中。
3.  **更新 (Update / M-step)**：重新计算每个簇的新均值（质心），将其作为新的簇中心。
4.  **迭代**：重复步骤2和3，直到簇中的对象不再发生变化，或者说准则函数收敛为止。

#### 局限性

* 可能终止于**局部最优解**。
* 需要**预先指定簇的数量k**，而k值在很多实际应用中难以确定。
* 对噪声和离群点数据**非常敏感**。
* 不适合发现**非凸形状**的簇，或大小差别很大的簇。

### 4.2 DBSCAN (基于密度)

#### 核心概念

DBSCAN将簇定义为由密度可达关系连接的、密度相连对象的最大集合。

* **ε-邻域**：给定对象半径ε内的区域。
* **核心对象**：如果一个对象的ε-邻域内至少包含MinPts个对象，则称该对象为核心对象。
* **直接密度可达**：如果点p在核心对象q的ε-邻域内，则p从q出发是直接密度可达的。
* **密度可达**：存在一个对象链，使得后一个对象是从前一个对象直接密度可达的。这是一个非对称关系。
* **密度相连**：如果存在一个核心对象o，使得p和q都从o密度可达，则p和q是密度相连的。这是一个对称关系。
* **噪声**：不包含在任何簇中的对象。

#### 算法步骤

1.  检查数据集中的每个点，判断其是否为核心对象。
2.  如果某个点是核心对象，就以它为起点创建一个新簇，并迭代地聚集所有从它直接或间接密度可达的对象。
3.  当没有新的点可以添加到任何簇时，算法结束。

#### 优缺点

* **优点**：可以有效地找到任意形状的簇，并且对噪声不敏感。
* **缺点**：对参数ε和MinPts的设置非常敏感，且对于密度不均的数据集效果不佳。

---

## 5. 聚类算法实验部分

### 实验一：K-Means对红酒数据集聚类

#### 1. 数据加载与标准化
```python
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_wine
# 加载数据
wine = load_wine()
# 标准化
scaler = StandardScaler()
X = scaler.fit_transform(wine.data)
y = wine.target
```

#### 2. 自定义K-Means实现
```python
import numpy as np

class K_means:
    def __init__(self, k=2, max_iterations=500, varepsilon=1e-4):
        self.k = k
        self.max_iterations = max_iterations
        self.epsilon = varepsilon
        
    def _euclidean_distance(self, point, centroids):
        """计算单个数据点到所有中心点的欧氏距离"""
        return np.sqrt(np.sum((point - centroids)**2, axis=1))

    def init_random_centroids(self, X):
        """从数据集中随机选择k个样本作为初始中心点"""
        n_samples, _ = X.shape
        random_indices = np.random.choice(n_samples, self.k, replace=False)
        return X[random_indices]
    
    def _closest_centroids(self, x, centroids):
        """计算单个样本所属的簇的 id"""
        distances = self._euclidean_distance(x, centroids)
        return np.argmin(distances)
    
    def create_clusters(self, centroids, X):
        """返回 clusters 数组，每个元素是一个簇的 样本id 数组"""
        clusters = [[] for _ in range(self.k)]
        for i, x in enumerate(X):
            cluster_id = self._closest_centroids(x, centroids)
            clusters[cluster_id].append(i)
        return [np.array(cluster) for cluster in clusters]
    
    def upd_centroids(self, clusters, X):
        n_features = X.shape[1]
        centroids = np.zeros((self.k, n_features))
        for i, cluster in enumerate(clusters):
            centroids[i] = np.mean(X[cluster], axis=0)
        return centroids
    
    # 将所有样本进行归类，labelid = 其所在的簇的 id
    def get_cluster_labels(self, clusters, X):
        labels = np.zeros(X.shape[0], dtype=int)
        for cluster_id, cluster in enumerate(clusters):
            for sample in cluster:
                labels[sample] = cluster_id
        return labels

    # 利用数据集 X 计算簇的中心点 （训练）,返回 X 中每个样本的簇 id
    def fit(self, X):
        centroids = self.init_random_centroids(X)
        for _ in range(self.max_iterations):
            clusters = self.create_clusters(centroids, X)
            new_centroids = self.upd_centroids(clusters, X)
            diff = np.abs(new_centroids - centroids)
            if diff.max() < self.epsilon:
                break
            centroids = new_centroids
        return self.get_cluster_labels(clusters, X)        
```

#### 3. 使用Sklearn的KMeans

```python
from sklearn.cluster import KMeans

def kmeans_cluster(data):
    km = KMeans(n_clusters=3, random_state=888)
    result = km.fit_predict(data)
    return result
```



### 实验二：DBSCAN对月亮数据集聚类

#### 1. 数据创建
```python
from sklearn import datasets

X, y = datasets.make_moons(n_samples=100, noise=0.005, random_state=114514)
```

#### 2. 自定义DBSCAN实现
```python
import numpy as np
import random

def findNeighbor(j, X, eps):
    N = []
    for p in range(X.shape[0]):
        temp = np.sqrt(np.sum(np.square(X[j] - X[p])))
        if(temp <= eps):
            N.append(p)
    return N

def dbscan(X, eps, min_Pts):
    k = -1
    fil = []
    gama = [x for x in range(len(X))]
    cluster = [-1 for y in range(len(X))]

    while len(gama) > 0:
        j = random.choice(gama)
        gama.remove(j)
        fil.append(j)
        
        NeighborPts = findNeighbor(j, X, eps)
        if len(NeighborPts) < min_Pts:
            cluster[j] = -1
        else:
            k = k + 1
            cluster[j] = k
            for i in NeighborPts:
                if i not in fil:
                    gama.remove(i)
                    fil.append(i)
                    Ner_NeighborPts = findNeighbor(i, X, eps)
                    if len(Ner_NeighborPts) >= min_Pts:
                        for a in Ner_NeighborPts:
                            if a not in NeighborPts:
                                NeighborPts.append(a)
                if (cluster[i] == -1):
                    cluster[i] = k
    return cluster
```

#### 3. 使用Sklearn的DBSCAN
```python
from sklearn.cluster import DBSCAN

dbscan_model = DBSCAN(eps=0.5, min_samples=10)
result = dbscan_model.fit_predict(X)
```