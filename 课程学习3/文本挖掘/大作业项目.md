# 项目 myNeuro

## 技术栈

前端 ：
HTML5 + JavaScript（WebRTC、WebGL、Web Audio API）。
Live2D Cubism SDK for WebGL（渲染角色）。
SoketIO库\WebSocket（实时通信） 与 http请求。



后端 ：
Python Flask（RESTful API + WebSocket）。
语音处理：Whisper（ASR）、Coqui TTS。
图像处理：OpenCV或Pillow。
多模态模型：Hugging Face Transformers（BERT + 情感分析）或自定义CLIP模型。
 （之后再说）实时推理优化：ONNX Runtime或TensorRT。



## 功能模块

前端 ：

实时语音采集与传输（WebRTC）。
摄像头捕捉图像（暂定，可选）
聊天框可以输入文本和图像
Live2D角色渲染与动态驱动（表情、动作、口型同步）。
与后端通过WebSocket通信，接收AI生成的对话和动作指令。



后端 ：

语音转文字（ASR）和情感分析。
多模态模型整合（结合语音文本、情感、表情生成回复）。
文本转语音（TTS）并返回给前端。



## 数据流

用户语音输入 → 前端通过WebSocket发送音频流 → 后端ASR转文字后生成回答并传回回答
后端多模态模型生成回复文本和动作指令 → 通过WebSocket返回前端。
前端驱动Live2D角色（表情、动作）并播放TTS语音。



## 模型训练与工程方面

**模型训练：**

- 角色性格塑造 -- 数据集选择（可能需要自己清洗数据）
- 多模态大模型挑选 7b 等模型，使用 lora + dpo 微调 （加上 dpo 提升一点工作量）
- TTS 模型训练微调，时间不够直接使用预训练模型
- ASR/STT 模型直接使用预训练模型即可



**工程方面：**

- 前端需学习 live2d SDK for web 的接口（能用就行）
- 后端使用 python Flask 实现路由管理，并且运行模型为前端提供服务
- 需要良好的项目结构（待构建）

